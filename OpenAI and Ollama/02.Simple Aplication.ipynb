{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cbb0448",
   "metadata": {},
   "source": [
    "## Simple Gen AI app using LangChain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af61989",
   "metadata": {},
   "source": [
    "### Load all the required keys\n",
    "\n",
    "What API keys do we need:\n",
    "- `OPENAI_API_KEY` - call OpenAI model LLM to generate response\n",
    "- `LANGCHAIN_API_KEY` - for collecting all the information in LangSmith\n",
    "- `LANGCHAIN_TRACING_V2` - for enabling the LangSmith tracing\n",
    "- `LANGCHAIN_PROJECT` - LangChain project name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b30366a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY: sk-proj-ID***tIA\n",
      "LANGCHAIN_API_KEY: lsv2_pt_64***43b\n",
      "LANGCHAIN_PROJECT: LangChain tutorial get started\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load the environment variables in .env file\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "print(f'OPENAI_API_KEY: {OPENAI_API_KEY[:10]}***{OPENAI_API_KEY[-3:]}')\n",
    "\n",
    "# We will use LANGCHAIN_API_KEY for LangSmith tracking\n",
    "LANGCHAIN_API_KEY = os.environ['LANGCHAIN_API_KEY']\n",
    "print(f'LANGCHAIN_API_KEY: {LANGCHAIN_API_KEY[:10]}***{LANGCHAIN_API_KEY[-3:]}')\n",
    "\n",
    "# Required by LangChain \n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "LANGCHAIN_PROJECT = os.environ['LANGCHAIN_PROJECT']\n",
    "print(f'LANGCHAIN_PROJECT: {LANGCHAIN_PROJECT}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269b6c93",
   "metadata": {},
   "source": [
    "### Retreve information from a website\n",
    "\n",
    "We will read the entire content of an website. For reading website content we will `beautifulsoup4` library. This library will help us **scrape** the entire website data. \n",
    "\n",
    "#### Data Ingestion\n",
    "\n",
    "Data Integestion means: From the entire website we will **scrape** the data.\n",
    "For loading the data into LangChain documents we will use `WebBaseLoader`. WebBaseLoader gets a URL link and uses `beautifulsoup4` to retrieve the data from it into a LangChain documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b78b81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\nHow to Sync Prompts with GitHub | 🦜️🛠️ LangSmith\\n\\n\\n\\n\\n\\n\\n\\n\\nSkip to main contentOur new LangChain Academy Course Deep Research with LangGraph is now live! Enroll for free.API ReferenceRESTPythonJS/TSSearchRegionUSEUGo to AppGet StartedObservabilityEvaluationPrompt EngineeringQuickstartsTutorialsOptimize a classifierSync Prompts with GitHubHow-to GuidesCreate a promptRun the playground against a custom LangServe model serverRun the playground against an OpenAI-compliant model provider/proxyUpdate a promptManage prompts programmaticallyManaging Prompt SettingsCommit TagsOpen a prompt from a tracePublic prompt hubPrompt CanvasInclude multimodal content in a promptTrigger a webhook on prompt commitUse tools in a promptHow to use multiple messages in the playgroundConceptual GuideDeployment (LangGraph Platform)AdministrationSelf-hostingPricingReferenceCloud architecture and scalabilityAuthz and AuthnAuthentication methodsdata_formatsEvaluationDataset transformationsRegions FAQsdk_referencePrompt EngineeringTutorialsSync Prompts with GitHubOn this pageHow to Sync Prompts with GitHub\\nLangSmith provides a collaborative interface to create, test, and iterate on prompts.\\nWhile you can dynamically fetch prompts from LangSmith into your application at runtime, you may prefer to sync prompts with your own database or version control system. To support this workflow, LangSmith allows you to receive notifications of prompt updates via webhooks.\\nWhy sync prompts with GitHub?\\n\\nVersion Control: Keep your prompts versioned alongside your application code in a familiar system.\\nCI/CD Integration: Trigger automated staging or production deployments when critical prompts change.\\n\\n\\nPrerequisites\\u200b\\nBefore we begin, ensure you have the following set up:\\n\\nGitHub Account: A standard GitHub account.\\nGitHub Repository: Create a new (or choose an existing) repository where your LangSmith prompt manifests will be stored. This could be the same repository as your application code or a dedicated one for prompts.\\nGitHub Personal Access Token (PAT):\\n\\nLangSmith webhooks don\\'t directly interact with GitHub—they call an intermediary server that you create.\\nThis server requires a GitHub PAT to authenticate and make commits to your repository.\\nMust include the repo scope (public_repo is sufficient for public repositories).\\nGo to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\\nClick Generate new token (classic).\\nName it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.\\nClick Generate token and copy it immediately — it won’t be shown again.\\nStore the token securely and provide it as an environment variable to your server.\\n\\n\\n\\nUnderstanding LangSmith \"Prompt Commits\" and Webhooks\\u200b\\nIn LangSmith, when you save changes to a prompt, you\\'re essentially creating a new version or a \"Prompt Commit.\" These commits are what can trigger webhooks.\\nThe webhook will send a JSON payload containing the new prompt manifest.\\nSample Webhook Payload{  \"prompt_id\": \"f33dcb51-eb17-47a5-83ca-64ac8a027a29\",  \"prompt_name\": \"My Prompt\",  \"commit_hash\": \"commit_hash_1234567890\",  \"created_at\": \"2021-01-01T00:00:00Z\",  \"created_by\": \"Jane Doe\",  \"manifest\": {    \"lc\": 1,    \"type\": \"constructor\",    \"id\": [\"langchain\", \"schema\", \"runnable\", \"RunnableSequence\"],    \"kwargs\": {      \"first\": {        \"lc\": 1,        \"type\": \"constructor\",        \"id\": [\"langchain\", \"prompts\", \"chat\", \"ChatPromptTemplate\"],        \"kwargs\": {          \"messages\": [            {              \"lc\": 1,              \"type\": \"constructor\",              \"id\": [                \"langchain_core\",                \"prompts\",                \"chat\",                \"SystemMessagePromptTemplate\"              ],              \"kwargs\": {                \"prompt\": {                  \"lc\": 1,                  \"type\": \"constructor\",                  \"id\": [                    \"langchain_core\",                    \"prompts\",                    \"prompt\",                    \"PromptTemplate\"                  ],                  \"kwargs\": {                    \"input_variables\": [],                    \"template_format\": \"mustache\",                    \"template\": \"You are a chatbot.\"                  }                }              }            },            {              \"lc\": 1,              \"type\": \"constructor\",              \"id\": [                \"langchain_core\",                \"prompts\",                \"chat\",                \"HumanMessagePromptTemplate\"              ],              \"kwargs\": {                \"prompt\": {                  \"lc\": 1,                  \"type\": \"constructor\",                  \"id\": [                    \"langchain_core\",                    \"prompts\",                    \"prompt\",                    \"PromptTemplate\"                  ],                  \"kwargs\": {                    \"input_variables\": [\"question\"],                    \"template_format\": \"mustache\",                    \"template\": \"{{question}}\"                  }                }              }            }          ],          \"input_variables\": [\"question\"]        }      },      \"last\": {        \"lc\": 1,        \"type\": \"constructor\",        \"id\": [\"langchain\", \"schema\", \"runnable\", \"RunnableBinding\"],        \"kwargs\": {          \"bound\": {            \"lc\": 1,            \"type\": \"constructor\",            \"id\": [\"langchain\", \"chat_models\", \"openai\", \"ChatOpenAI\"],            \"kwargs\": {              \"temperature\": 1,              \"top_p\": 1,              \"presence_penalty\": 0,              \"frequency_penalty\": 0,              \"model\": \"gpt-4.1-mini\",              \"extra_headers\": {},              \"openai_api_key\": {                \"id\": [\"OPENAI_API_KEY\"],                \"lc\": 1,                \"type\": \"secret\"              }            }          },          \"kwargs\": {}        }      }    }  }}\\nWorkspace Level TriggersIt\\'s important to understand that LangSmith webhooks for prompt commits are generally triggered at the workspace level. This means if any prompt within your LangSmith workspace is modified and a \"prompt commit\" is saved, the webhook will fire and send the updated manifest of the prompt. The payloads are identifiable by prompt id. Your receiving server should be designed with this in mind.\\nImplementing a FastAPI Server for Webhook Reception\\u200b\\nTo effectively process webhook notifications from LangSmith when prompts are updated, an intermediary server application is necessary. This server will act as the receiver for HTTP POST requests sent by LangSmith. For demonstration purposes in this guide, we will outline the creation of a simple FastAPI application to fulfill this role.\\nThis publicly accessible server will be responsible for:\\n\\nReceiving Webhook Requests: Listening for incoming HTTP POST requests.\\nParsing Payloads: Extracting and interpreting the JSON-formatted prompt manifest from the request body.\\nCommitting to GitHub: Programmatically creating a new commit in your specified GitHub repository, containing the updated prompt manifest. This ensures your prompts remain version-controlled and synchronized with changes made in LangSmith.\\n\\nFor deployment, platforms like Render.com (offering a suitable free tier), Vercel, Fly.io, or other cloud providers (AWS, GCP, Azure) can be utilized to host the FastAPI application and obtain a public URL.\\nThe server\\'s core functionality will include an endpoint for webhook reception, logic for parsing the manifest, and integration with the GitHub API (using a Personal Access Token for authentication) to manage commits.\\nMinimal FastAPI Server Code (main.py)This server will listen for incoming webhooks from LangSmith and commit the received prompt manifest to your GitHub repository.import base64import jsonimport uuidfrom typing import Any, Dictimport httpxfrom fastapi import FastAPI, HTTPException, Bodyfrom pydantic import BaseModel, Fieldfrom pydantic_settings import BaseSettings, SettingsConfigDict# --- Configuration ---class AppConfig(BaseSettings):    \"\"\"    Application configuration model.    Loads settings from environment variables.    \"\"\"    GITHUB_TOKEN: str    GITHUB_REPO_OWNER: str    GITHUB_REPO_NAME: str    GITHUB_FILE_PATH: str = \"prompt_manifest.json\"    GITHUB_BRANCH: str = \"main\"    model_config = SettingsConfigDict(        env_file=\".env\",        env_file_encoding=\\'utf-8\\',        extra=\\'ignore\\'    )settings = AppConfig()# --- Pydantic Models ---class WebhookPayload(BaseModel):    \"\"\"    Defines the expected structure of the incoming webhook payload.    \"\"\"    prompt_id: UUID = Field(        ...,        description=\"The unique identifier for the prompt.\"    )    prompt_name: str = Field(        ...,        description=\"The name/title of the prompt.\"    )    commit_hash: str = Field(        ...,        description=\"An identifier for the commit event that triggered the webhook.\"    )    created_at: str = Field(        ...,        description=\"Timestamp indicating when the event was created (ISO format preferred).\"    )    created_by: str = Field(        ...,        description=\"The name of the user who created the event.\"    )    manifest: Dict[str, Any] = Field(        ...,        description=\"The main content or configuration data to be committed to GitHub.\"    )# --- GitHub Helper Function ---async def commit_manifest_to_github(payload: WebhookPayload) -> Dict[str, Any]:    \"\"\"    Helper function to commit the manifest directly to the configured branch.    \"\"\"    github_api_base_url = \"https://api.github.com\"    repo_file_url = (        f\"{github_api_base_url}/repos/{settings.GITHUB_REPO_OWNER}/\"        f\"{settings.GITHUB_REPO_NAME}/contents/{settings.GITHUB_FILE_PATH}\"    )    headers = {        \"Authorization\": f\"Bearer {settings.GITHUB_TOKEN}\",        \"Accept\": \"application/vnd.github.v3+json\",        \"X-GitHub-Api-Version\": \"2022-11-28\",    }    manifest_json_string = json.dumps(payload.manifest, indent=2)    content_base64 = base64.b64encode(manifest_json_string.encode(\\'utf-8\\')).decode(\\'utf-8\\')    commit_message = f\"feat: Update {settings.GITHUB_FILE_PATH} via webhook - commit {payload.commit_hash}\"    data_to_commit = {        \"message\": commit_message,        \"content\": content_base64,        \"branch\": settings.GITHUB_BRANCH,    }    async with httpx.AsyncClient() as client:        current_file_sha = None        try:            params_get = {\"ref\": settings.GITHUB_BRANCH}            response_get = await client.get(repo_file_url, headers=headers, params=params_get)            if response_get.status_code == 200:                current_file_sha = response_get.json().get(\"sha\")            elif response_get.status_code != 404: # If not 404 (not found), it\\'s an unexpected error                response_get.raise_for_status()        except httpx.HTTPStatusError as e:            error_detail = f\"GitHub API error (GET file SHA): {e.response.status_code} - {e.response.text}\"            print(f\"[ERROR] {error_detail}\")            raise HTTPException(status_code=e.response.status_code, detail=error_detail)        except httpx.RequestError as e:            error_detail = f\"Network error connecting to GitHub (GET file SHA): {str(e)}\"            print(f\"[ERROR] {error_detail}\")            raise HTTPException(status_code=503, detail=error_detail)        if current_file_sha:            data_to_commit[\"sha\"] = current_file_sha        try:            response_put = await client.put(repo_file_url, headers=headers, json=data_to_commit)            response_put.raise_for_status()            return response_put.json()        except httpx.HTTPStatusError as e:            error_detail = f\"GitHub API error (PUT content): {e.response.status_code} - {e.response.text}\"            if e.response.status_code == 409: # Conflict                error_detail = (                    f\"GitHub API conflict (PUT content): {e.response.text}. \"                    \"This might be due to an outdated SHA or branch protection rules.\"                )            elif e.response.status_code == 422: # Unprocessable Entity                 error_detail = (                    f\"GitHub API Unprocessable Entity (PUT content): {e.response.text}. \"                    f\"Ensure the branch \\'{settings.GITHUB_BRANCH}\\' exists and the payload is correctly formatted.\"                )            print(f\"[ERROR] {error_detail}\")            raise HTTPException(status_code=e.response.status_code, detail=error_detail)        except httpx.RequestError as e:            error_detail = f\"Network error connecting to GitHub (PUT content): {str(e)}\"            print(f\"[ERROR] {error_detail}\")            raise HTTPException(status_code=503, detail=error_detail)# --- FastAPI Application ---app = FastAPI(    title=\"Minimal Webhook to GitHub Commit Service\",    description=\"Receives a webhook and commits its \\'manifest\\' part directly to a GitHub repository.\",    version=\"0.1.0\",)@app.post(\"/webhook/commit\", status_code=201, tags=[\"GitHub Webhooks\"])async def handle_webhook_direct_commit(payload: WebhookPayload = Body(...)):    \"\"\"    Webhook endpoint to receive events and commit DIRECTLY to the configured branch.    \"\"\"    try:        github_response = await commit_manifest_to_github(payload)        return {            \"message\": \"Webhook received and manifest committed directly to GitHub successfully.\",            \"github_commit_details\": github_response.get(\"commit\", {}),            \"github_content_details\": github_response.get(\"content\", {})        }    except HTTPException:        raise # Re-raise if it\\'s an HTTPException from the helper    except Exception as e:        error_message = f\"An unexpected error occurred: {str(e)}\"        print(f\"[ERROR] {error_message}\")        raise HTTPException(status_code=500, detail=\"An internal server error occurred.\")@app.get(\"/health\", status_code=200, tags=[\"Health\"])async def health_check():    \"\"\"    A simple health check endpoint.    \"\"\"    return {\"status\": \"ok\", \"message\": \"Service is running.\"}# To run this server (save as main.py):# 1. Install dependencies: pip install fastapi uvicorn pydantic pydantic-settings httpx python-dotenv# 2. Create a .env file with your GitHub token and repo details.# 3. Run with Uvicorn: uvicorn main:app --reload# 4. Deploy to a public platform like Render.com.Key aspects of this server:\\nConfiguration (.env): It expects a .env file with your GITHUB_TOKEN, GITHUB_REPO_OWNER, and GITHUB_REPO_NAME. You can also customize GITHUB_FILE_PATH (default: LangSmith_prompt_manifest.json) and GITHUB_BRANCH (default: main).\\nGitHub Interaction: The commit_manifest_to_github function handles the logic of fetching the current file\\'s SHA (to update it) and then committing the new manifest content.\\nWebhook Endpoint (/webhook/commit): This is the URL path your LangSmith webhook will target.\\nError Handling: Basic error handling for GitHub API interactions is included.\\nDeploy this server to your chosen platform (e.g., Render) and note down its public URL (e.g., https://prompt-commit-webhook.onrender.com).\\nConfiguring the Webhook in LangSmith\\u200b\\nOnce your FastAPI server is deployed and you have its public URL, you can configure the webhook in LangSmith:\\n\\n\\nNavigate to your LangSmith workspace.\\n\\n\\nGo to the Prompts section. Here you\\'ll see a list of your prompts.\\n\\n\\n\\nOn the top right of the Prompts page, click the + Webhook button.\\n\\n\\nYou\\'ll be presented with a form to configure your webhook:\\n\\n\\nWebhook URL: Enter the full public URL of your deployed FastAPI server\\'s endpoint. For our example server, this would be https://prompt-commit-webhook.onrender.com/webhook/commit.\\nHeaders (Optional):\\n\\nYou can add custom headers that LangSmith will send with each webhook request.\\n\\n\\n\\n\\n\\nTest the Webhook: LangSmith provides a \"Send Test Notification\" button. Use this to send a sample payload to your server. Check your server logs (e.g., on Render) to ensure it receives the request and processes it successfully (or to debug any issues).\\n\\n\\nSave the webhook configuration.\\n\\n\\nThe Workflow in Action\\u200b\\n\\nNow, with everything set up, here\\'s what happens:\\n\\n\\nPrompt Modification: A user (developer or non-technical team member) modifies a prompt in the LangSmith UI and saves it, creating a new \"prompt commit.\"\\n\\n\\nWebhook Trigger: LangSmith detects this new prompt commit and triggers the configured webhook.\\n\\n\\nHTTP Request: LangSmith sends an HTTP POST request to the public URL of your FastAPI server (e.g., https://prompt-commit-webhook.onrender.com/webhook/commit). The body of this request contains the JSON prompt manifest for the entire workspace.\\n\\n\\nServer Receives Payload: Your FastAPI server\\'s endpoint receives the request.\\n\\n\\nGitHub Commit: The server parses the JSON manifest from the request body. It then uses the configured GitHub Personal Access Token, repository owner, repository name, file path, and branch to:\\n\\nCheck if the manifest file already exists in the repository on the specified branch to get its SHA (this is necessary for updating an existing file).\\nCreate a new commit with the latest prompt manifest, either creating the file or updating it if it already exists. The commit message will indicate that it\\'s an update from LangSmith.\\n\\n\\n\\nConfirmation: You should see the new commit appear in your GitHub repository.\\n\\n\\n\\nYou\\'ve now successfully synced your LangSmith prompts with GitHub!\\nBeyond a Simple Commit\\u200b\\nOur example FastAPI server performs a direct commit of the entire prompt manifest. However, this is just the starting point. You can extend the server\\'s functionality to perform more sophisticated actions:\\n\\nGranular Commits: Parse the manifest and commit changes to individual prompt files if you prefer a more granular structure in your repository.\\nTrigger CI/CD: Instead of (or in addition to) committing, have the server trigger a CI/CD pipeline (e.g., Jenkins, GitHub Actions, GitLab CI) to deploy a staging environment, run tests, or build new application versions.\\nUpdate Databases/Caches: If your application loads prompts from a database or cache, update these stores directly.\\nNotifications: Send notifications to Slack, email, or other communication channels about prompt changes.\\nSelective Processing: Based on metadata within the LangSmith payload (if available, e.g., which specific prompt changed or by whom), you could apply different logic.\\nWas this page helpful?You can leave detailed feedback on GitHub.PreviousOptimize a classifierNextPrompt engineering how-to guidesPrerequisitesUnderstanding LangSmith \"Prompt Commits\" and WebhooksImplementing a FastAPI Server for Webhook ReceptionConfiguring the Webhook in LangSmithThe Workflow in ActionBeyond a Simple CommitCommunityLangChain ForumTwitterGitHubDocs CodeLangSmith SDKPythonJS/TSMoreHomepageBlogLangChain Python DocsLangChain JS/TS DocsCopyright © 2025 LangChain, Inc.\\n\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "url = 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit'\n",
    "\n",
    "web_loader = WebBaseLoader(web_path=url)\n",
    "documents = web_loader.load()\n",
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b89d3b9",
   "metadata": {},
   "source": [
    "We loaded all the documents. But it is huge content containing a lot of information. We need to divide it into **chunks**. We cannot give the entire content to the LLM, because there is a limitation for the maximum context size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58df9b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitted documents size: 50\n",
      "Minimun document size: 48\n",
      "Maximum document size: 498\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splitted_documents = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f'Splitted documents size: {len(splitted_documents)}')\n",
    "min_chunk_size = min([len(s.page_content) for s in splitted_documents])\n",
    "max_chunk_size = max([len(s.page_content) for s in splitted_documents])\n",
    "\n",
    "print(f'Minimun document size: {min_chunk_size}')\n",
    "print(f'Maximum document size: {max_chunk_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a58e2ff",
   "metadata": {},
   "source": [
    "### Converting documents into vectors\n",
    "\n",
    "When we work with Q&A chatbot or document Q&A or RAG application, we use **cosine similarity** to provide **context** to the LLM.\n",
    "To convert our documents into vectors, we need an embedding technique. For this tutorial we will use `OpenAI`.\n",
    "\n",
    "After we embed all the documents we will store them in **FAISS** database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e4ea19a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-3-small'\n",
    "embeddings = OpenAIEmbeddings(model=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fa6d29c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading FAISS database from ./db/faiss with index name simple-app-faiss-store\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import faiss\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.docstore.in_memory import InMemoryDocstore\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "\n",
    "db_folder = './db/faiss'\n",
    "faiss_db_index = 'simple-app-faiss-store'\n",
    "\n",
    "index_path = os.path.join(db_folder, f'{faiss_db_index}.faiss')\n",
    "\n",
    "if not os.path.exists(index_path):\n",
    "    print(f'Creating a FAISS database in dir {db_folder} and index name {faiss_db_index}')\n",
    "    index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "\n",
    "    faiss_db = FAISS(embeddings,\n",
    "                    index=index,\n",
    "                    docstore=InMemoryDocstore(),\n",
    "                    index_to_docstore_id={},\n",
    "                    distance_strategy=DistanceStrategy.COSINE)\n",
    "else:\n",
    "    print(f'Loading FAISS database from {db_folder} with index name {faiss_db_index}')\n",
    "    faiss_db = FAISS.load_local(folder_path=db_folder, embeddings=embeddings,\n",
    "                                index_name=faiss_db_index,\n",
    "                                allow_dangerous_deserialization=True)\n",
    "    \n",
    "faiss_db.add_documents(splitted_documents)\n",
    "faiss_db.save_local(folder_path=db_folder, index_name=faiss_db_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f4c744",
   "metadata": {},
   "source": [
    "## Lets query from the **FAISS** database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "132cc257",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity 1: page_content='LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that you create.\n",
      "This server requires a GitHub PAT to authenticate and make commits to your repository.\n",
      "Must include the repo scope (public_repo is sufficient for public repositories).\n",
      "Go to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\n",
      "Click Generate new token (classic).\n",
      "Name it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.' metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}\n",
      "Score: 0.4748978614807129\n",
      "\n",
      "\n",
      "Similarity 2: page_content='LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that you create.\n",
      "This server requires a GitHub PAT to authenticate and make commits to your repository.\n",
      "Must include the repo scope (public_repo is sufficient for public repositories).\n",
      "Go to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\n",
      "Click Generate new token (classic).\n",
      "Name it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.' metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}\n",
      "Score: 0.4750254154205322\n",
      "\n",
      "\n",
      "Similarity 3: page_content='LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that you create.\n",
      "This server requires a GitHub PAT to authenticate and make commits to your repository.\n",
      "Must include the repo scope (public_repo is sufficient for public repositories).\n",
      "Go to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\n",
      "Click Generate new token (classic).\n",
      "Name it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.' metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}\n",
      "Score: 0.4750254154205322\n",
      "\n",
      "\n",
      "Similarity 4: page_content='Configuration (.env): It expects a .env file with your GITHUB_TOKEN, GITHUB_REPO_OWNER, and GITHUB_REPO_NAME. You can also customize GITHUB_FILE_PATH (default: LangSmith_prompt_manifest.json) and GITHUB_BRANCH (default: main).\n",
      "GitHub Interaction: The commit_manifest_to_github function handles the logic of fetching the current file's SHA (to update it) and then committing the new manifest content.\n",
      "Webhook Endpoint (/webhook/commit): This is the URL path your LangSmith webhook will target.' metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}\n",
      "Score: 0.5879706740379333\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = 'How LangSmith webhooks interact with GitHub?'\n",
    "\n",
    "K = 4\n",
    "\n",
    "result = faiss_db.similarity_search_with_score(query=query, k=K)\n",
    "for i, (sim, score) in enumerate(result):\n",
    "    print(f'Similarity {i + 1}: {sim}')\n",
    "    print(f'Score: {score}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e57b6",
   "metadata": {},
   "source": [
    "### Create LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b68857d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "client=<openai.resources.chat.completions.completions.Completions object at 0x10f3be120> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f3be510> root_client=<openai.OpenAI object at 0x10f2796d0> root_async_client=<openai.AsyncOpenAI object at 0x10f27afd0> model_name='gpt-4o' model_kwargs={} openai_api_key=SecretStr('**********')\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm = ChatOpenAI(model='gpt-4o', api_key=OPENAI_API_KEY)\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e28764",
   "metadata": {},
   "source": [
    "### Create a **Retrieval chain**\n",
    "\n",
    "To pass all the documents to the LLM we need to provide all the retrieved documents. For that purpose we will use `create_stuff_documents_chain` method from `langchain.chains.combine_documents` package.\n",
    "\n",
    "Document chain will be responsible for providing the prompt template {context} input parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2132a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents chain: bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
      "  context: RunnableLambda(format_docs)\n",
      "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
      "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context> {context} </context>\\n'), additional_kwargs={})])\n",
      "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10f3be120>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f3be510>, root_client=<openai.OpenAI object at 0x10f2796d0>, root_async_client=<openai.AsyncOpenAI object at 0x10f27afd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
      "| StrOutputParser() kwargs={} config={'run_name': 'stuff_documents_chain'} config_factories=[]\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context> {context} </context>\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=prompt_template)\n",
    "\n",
    "documents_chain = create_stuff_documents_chain(llm=llm, prompt=prompt)\n",
    "print(f'Documents chain: {documents_chain}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b55dfa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calling LLM with query: \"How LangSmith webhooks interact with GitHub?\" ...\n",
      "Result:\n",
      "How do LangSmith webhooks interact with GitHub repositories according to the provided context?\n",
      "\n",
      "LangSmith webhooks do not interact directly with GitHub repositories. Instead, they call an intermediary server, which you must create. This server uses a GitHub Personal Access Token (PAT) with at least the `public_repo` scope to authenticate and make commits to your repository.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "print(f'Calling LLM with query: \"{query}\" ...')\n",
    "result = documents_chain.invoke(input={\n",
    "    'input': query,\n",
    "    'context': [Document(page_content=\"\"\"LangSmith webhooks don't directly interact with GitHub—they call an intermediary server that you create.\n",
    "This server requires a GitHub PAT to authenticate and make commits to your repository.\n",
    "Must include the repo scope (public_repo is sufficient for public repositories).\"\"\")]\n",
    "})\n",
    "\n",
    "print('Result:')\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b34dfb",
   "metadata": {},
   "source": [
    "However, we want the documents to first come from the retriever we just setup. That way, we can use the retriever to dynamically select the most relevant documents and pass those in for a given question.\n",
    "\n",
    "But what exactly is a **Retriever**?<br><br>\n",
    "We have a Vector store DB. This Vector store has all the vector information available right now. Retriever can be considered as an interface, which is responsible for if anybody asks a given input, this interface will be a way of getting the data from the Vector store.\n",
    "<br><br>\n",
    "Each vectorstorstore DB has a method `as_retriever()` which returns the datastore as a Retriever interface.\n",
    "<br><br>\n",
    "From the documentation: `Return VectorStoreRetriever initialized from this VectorStore.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "316e4089",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x10f1e3b60>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context> {context} </context>\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x10f3be120>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x10f3be510>, root_client=<openai.OpenAI object at 0x10f2796d0>, root_async_client=<openai.AsyncOpenAI object at 0x10f27afd0>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains.retrieval import create_retrieval_chain\n",
    "\n",
    "retriever = faiss_db.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever=retriever, combine_docs_chain=documents_chain)\n",
    "retrieval_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c4e970a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'How LangSmith webhooks interact with GitHub?',\n",
       " 'context': [Document(id='fa467293-55bb-423d-b0de-a6475833be95', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}, page_content='LangSmith webhooks don\\'t directly interact with GitHub—they call an intermediary server that you create.\\nThis server requires a GitHub PAT to authenticate and make commits to your repository.\\nMust include the repo scope (public_repo is sufficient for public repositories).\\nGo to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\\nClick Generate new token (classic).\\nName it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.'),\n",
       "  Document(id='a1f3d399-8733-4877-9763-6fe48dcb9f52', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}, page_content='LangSmith webhooks don\\'t directly interact with GitHub—they call an intermediary server that you create.\\nThis server requires a GitHub PAT to authenticate and make commits to your repository.\\nMust include the repo scope (public_repo is sufficient for public repositories).\\nGo to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\\nClick Generate new token (classic).\\nName it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.'),\n",
       "  Document(id='f845dfb7-5571-4453-95ef-0bf6e96df615', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}, page_content='LangSmith webhooks don\\'t directly interact with GitHub—they call an intermediary server that you create.\\nThis server requires a GitHub PAT to authenticate and make commits to your repository.\\nMust include the repo scope (public_repo is sufficient for public repositories).\\nGo to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic).\\nClick Generate new token (classic).\\nName it (e.g., \"LangSmith Prompt Sync\"), set an expiration, and select the required scopes.'),\n",
       "  Document(id='24cdea68-728f-45b7-821e-b99dbdc757ae', metadata={'source': 'https://docs.smith.langchain.com/prompt_engineering/tutorials/prompt_commit', 'title': 'How to Sync Prompts with GitHub | 🦜️🛠️ LangSmith', 'description': 'LangSmith provides a collaborative interface to create, test, and iterate on prompts.', 'language': 'en'}, page_content=\"Configuration (.env): It expects a .env file with your GITHUB_TOKEN, GITHUB_REPO_OWNER, and GITHUB_REPO_NAME. You can also customize GITHUB_FILE_PATH (default: LangSmith_prompt_manifest.json) and GITHUB_BRANCH (default: main).\\nGitHub Interaction: The commit_manifest_to_github function handles the logic of fetching the current file's SHA (to update it) and then committing the new manifest content.\\nWebhook Endpoint (/webhook/commit): This is the URL path your LangSmith webhook will target.\")],\n",
       " 'answer': 'What is necessary to authenticate and make commits to a GitHub repository according to the provided context?\\n\\nTo authenticate and make commits to a GitHub repository, you need to create an intermediary server and use a GitHub Personal Access Token (PAT). The PAT must include the \"repo\" scope, and \"public_repo\" is sufficient for public repositories. You generate this token by going to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic), clicking \"Generate new token (classic),\" naming it (e.g., \"LangSmith Prompt Sync\"), setting an expiration, and selecting the required scopes. Additionally, you need to configure a .env file with your GITHUB_TOKEN, GITHUB_REPO_OWNER, GITHUB_REPO_NAME, and optionally customize GITHUB_FILE_PATH and GITHUB_BRANCH.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved = retrieval_chain.invoke(input={'input': query})\n",
    "retrieved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd5395a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is necessary to authenticate and make commits to a GitHub repository according to the provided context?\n",
      "\n",
      "To authenticate and make commits to a GitHub repository, you need to create an intermediary server and use a GitHub Personal Access Token (PAT). The PAT must include the \"repo\" scope, and \"public_repo\" is sufficient for public repositories. You generate this token by going to GitHub > Settings > Developer settings > Personal access tokens > Tokens (classic), clicking \"Generate new token (classic),\" naming it (e.g., \"LangSmith Prompt Sync\"), setting an expiration, and selecting the required scopes. Additionally, you need to configure a .env file with your GITHUB_TOKEN, GITHUB_REPO_OWNER, GITHUB_REPO_NAME, and optionally customize GITHUB_FILE_PATH and GITHUB_BRANCH.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee62b5",
   "metadata": {},
   "source": [
    "w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399a72fc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
